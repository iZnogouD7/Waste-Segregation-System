{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AWyiJ6PYiIDI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing import image as keras_image\n",
        "from colabcode import ColabCode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_-xg1A5iLPF",
        "outputId": "0da108d9-8df4-4619-a181-0266d9b5fa03"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'mount_drive' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Mount Google Drive to access the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmount_drive\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Update path accordingly\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'mount_drive' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# Mount Google Drive to access the dataset\n",
        "mount_drive('/content/drive')  # Update path accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy_bZoZ-iLLw"
      },
      "outputs": [],
      "source": [
        "# Define paths to your dataset folders\n",
        "train_path = '/content/drive/MyDrive/Garbage/Train'\n",
        "test_path = '/content/drive/MyDrive/Garbage/Test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf8DM8zgiLIT"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ImageDataGenerator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess and augment the images\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m \u001b[43mImageDataGenerator\u001b[49m(\n\u001b[0;32m      3\u001b[0m     rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m,\n\u001b[0;32m      4\u001b[0m     shear_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m      5\u001b[0m     zoom_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m      6\u001b[0m     horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n",
            "\u001b[1;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
          ]
        }
      ],
      "source": [
        "# Preprocess and augment the images\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1opg-eHiLFw",
        "outputId": "726a4546-8302-4751-cfa2-da7c318edee1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ImageDataGenerator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_datagen \u001b[38;5;241m=\u001b[39m \u001b[43mImageDataGenerator\u001b[49m(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m      3\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m      4\u001b[0m     train_path,\n\u001b[0;32m      5\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[0;32m      6\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      7\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n",
            "\u001b[1;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
          ]
        }
      ],
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux-sO1TuiLBU",
        "outputId": "01629897-c570-4f31-b9b3-645eba150198"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_datagen' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m \u001b[43mtest_datagen\u001b[49m\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m      2\u001b[0m     test_path,\n\u001b[0;32m      3\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[0;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      5\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#classes = list(validation_generator.class_indices.keys())\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'test_datagen' is not defined"
          ]
        }
      ],
      "source": [
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "#classes = list(validation_generator.class_indices.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldFySx1biK92"
      },
      "outputs": [],
      "source": [
        "# Load VGG16 model without top layers\n",
        "vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in vgg.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HAcoMg3Zjnq",
        "outputId": "ba1e0247-c82f-4dcb-c16d-2b0293778448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vgg.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6uNRdyYiK57"
      },
      "outputs": [],
      "source": [
        "# Create a new model\n",
        "model = Sequential()\n",
        "model.add(vgg)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(3, activation='softmax'))  # 6 classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u127GzrNiK2Q"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=1.0e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmvfN1aVNK-m",
        "outputId": "4ee4cd67-40aa-494e-f835-22b91662e142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               12845568  \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 512)               2048      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 27563843 (105.15 MB)\n",
            "Trainable params: 12848131 (49.01 MB)\n",
            "Non-trainable params: 14715712 (56.14 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# View structure of the model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC5oterdiKxo",
        "outputId": "761cd56b-5b52-4ceb-a4bb-20ce79718b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.8932 - accuracy: 0.6321 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r83/83 [==============================] - 2699s 33s/step - loss: 0.8932 - accuracy: 0.6321 - val_loss: 0.7741 - val_accuracy: 0.6039\n",
            "Epoch 2/50\n",
            " 6/83 [=>............................] - ETA: 27:53 - loss: 0.7571 - accuracy: 0.7240"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // 32,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // 32,\n",
        "    epochs=50,  # Update the number of epochs as needed\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJE1i4zFQr1p"
      },
      "outputs": [],
      "source": [
        "# Assuming you have loaded your test data and model as previously described\n",
        "# Load and preprocess test data\n",
        "#test_data = ...  # Load and preprocess your test data similar to the training data\n",
        "classes = [ 'plastic', 'paper','metal']\n",
        "# Make predictions using the trained model on test data\n",
        "predictions1 = model.predict(validation_generator)\n",
        "\n",
        "\n",
        "\n",
        "# Decode the predicted classes (assuming 'classes' is a list of class labels)\n",
        "predicted_labels1 = [classes[np.argmax(pred)] for pred in predictions1]\n",
        "\n",
        "\n",
        "# Obtain true labels for test data\n",
        "true_labels1 = validation_generator.classes  # Assuming you used ImageDataGenerator for test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbobdfr9Q6JG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "# Convert class indices to class names (if true_labels are integers)\n",
        "#true_labels1 = [classes[label] for label in true_labels1]\n",
        "true_labels1 = [classes[label_index] for label_index in validation_generator.classes]\n",
        "\n",
        "# Obtain true labels for the training data\n",
        "# Or, convert class names to class indices (if predicted_labels are strings)\n",
        "#predicted_labels1 = [classes[label] for label in predicted_labels1]\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix_test = confusion_matrix(true_labels1, predicted_labels1)\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix for test:\")\n",
        "print(conf_matrix_test)\n",
        "\n",
        "# Calculate classification metrics\n",
        "accuracy1 = accuracy_score(true_labels1, predicted_labels1)\n",
        "classification_rep1 = classification_report(true_labels1, predicted_labels1)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(f\"\\nAccuracy: {accuracy1 * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kQc3IXcRG3K"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot confusion matrix as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_test, annot=True, cmap='Blues', fmt='g',xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted labels for test')\n",
        "plt.ylabel('True labels for test')\n",
        "plt.title('Confusion Matrix for test')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIEUwQi6hyx-"
      },
      "outputs": [],
      "source": [
        "#predictions2 = model.predict(train_generator)\n",
        "#predicted_labels2 = [classes[np.argmax(pred)] for pred in predictions2]\n",
        "#true_labels2 = train_generator.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcqRlyYt1uO2"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the training data\n",
        "predictions_train = model.predict(train_generator)\n",
        "\n",
        "# Decode predictions into class labels\n",
        "predicted_labels_train = [classes[np.argmax(pred)] for pred in predictions_train]\n",
        "\n",
        "# Obtain true labels for the training data\n",
        "# Load specific class labels into true_labels_train\n",
        "specified_classes = ['plastic', 'paper', 'metal']  # Replace with your specified classes\n",
        "true_labels_train = [classes[label_index] for label_index in train_generator.classes]\n",
        "#true_labels_train = [classes[label_index] for label_index in train_generator.classes if classes[label_index] in specified_classes]\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix_train = confusion_matrix(true_labels_train, predicted_labels_train, labels=specified_classes)\n",
        "\n",
        "print(\"Confusion Matrix for training data:\")\n",
        "print(conf_matrix_train)\n",
        "accuracy2 = accuracy_score(true_labels_train, predicted_labels_train)\n",
        "classification_rep2 = classification_report(true_labels_train, predicted_labels_train, target_names=specified_classes)\n",
        "\n",
        "positive_classes = ['plastic', 'paper']\n",
        "negative_class = 'metal'\n",
        "\n",
        "# Convert 3x3 matrix to 2x2 matrix\n",
        "conf_matrix_train1 = confusion_matrix(true_labels_train, predicted_labels_train, labels=[negative_class] + positive_classes)\n",
        "\n",
        "# Calculate True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)\n",
        "TN = conf_matrix_train[0, 0]\n",
        "FP = conf_matrix_train[0, 1]\n",
        "FN = conf_matrix_train[1, 0]\n",
        "TP = conf_matrix_train[1, 1]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy8 = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "print(\"Confusion Matrix for training data (2x2):\")\n",
        "print(conf_matrix_train1)\n",
        "print(\"\\nAccuracy:\", accuracy8)\n",
        "# Print accuracy and classification report\n",
        "print(f\"\\nAccuracy: {accuracy2 * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UFe53e0hyl_"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW5WIonuiqBJ"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_train, annot=True, cmap='Blues', fmt='g', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted labels for train')\n",
        "plt.ylabel('True labels for train')\n",
        "plt.title('Confusion Matrix for train')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f58Kf2uhX8T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TaryZgMbOQ6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Assuming you have obtained true_labels and predicted_labels as described earlier\n",
        "# Calculate classification report\n",
        "class_rep = classification_report(true_labels1, predicted_labels1, target_names=classes, output_dict=True)\n",
        "\n",
        "# Extract precision, recall, and F1-score from the classification report\n",
        "precision = [class_rep[label]['precision'] for label in classes]\n",
        "recall = [class_rep[label]['recall'] for label in classes]\n",
        "f1_score = [class_rep[label]['f1-score'] for label in classes]\n",
        "\n",
        "# Plotting precision, recall, and F1-score\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.bar(classes, precision, alpha=0.5, color='blue', label='Precision')\n",
        "plt.bar(classes, recall, alpha=0.5, color='red', label='Recall')\n",
        "plt.bar(classes, f1_score, alpha=0.5, color='green', label='F1-score')\n",
        "\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Precision, Recall, and F1-score by Class for validation')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J4LRTUli8_e"
      },
      "outputs": [],
      "source": [
        "class_rep2 = classification_report(true_labels_train, predicted_labels_train, target_names=classes, output_dict=True)\n",
        "\n",
        "# Extract precision, recall, and F1-score from the classification report\n",
        "precision2 = [class_rep[label]['precision'] for label in classes]\n",
        "recall2 = [class_rep[label]['recall'] for label in classes]\n",
        "f1_score2 = [class_rep[label]['f1-score'] for label in classes]\n",
        "\n",
        "# Plotting precision, recall, and F1-score\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.bar(classes, precision2, alpha=0.5, color='blue', label='Precision')\n",
        "plt.bar(classes, recall2, alpha=0.5, color='red', label='Recall')\n",
        "plt.bar(classes, f1_score2, alpha=0.5, color='green', label='F1-score')\n",
        "\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Precision, Recall, and F1-score by Class for train')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukNvJHVgN6jF"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Plotting the training and validation accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "model.save('/content/drive/MyDrive/Garbage/model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEqEfr7TiKkI"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "loss99, accuracy99 = model.evaluate(validation_generator)\n",
        "print(f\"Validation Accuracy: {accuracy99 * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQrvNGSFaZ5N"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "loss55, accuracy55 = model.evaluate(train_generator)\n",
        "print(f\"Training Accuracy: {accuracy55 * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U9FyTxd40ch"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def load_image(img_path):\n",
        "    img = Image.open(img_path)\n",
        "    img = img.resize((224, 224))\n",
        "    img = keras_image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img /= 255.0  # Normalization\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZTBA8MbsQAi"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "new_image = load_image('/content/drive/MyDrive/Garbage/photos/Bill.jpg')\n",
        "\n",
        "prediction = model.predict(new_image)\n",
        "classes = ['metal', 'paper','plastic']\n",
        "predicted_class_index = np.argmax(prediction)\n",
        "predicted_class = classes[predicted_class_index]\n",
        "test_input = new_image.reshape((224,224,3))\n",
        "plt.imshow(test_input)\n",
        "\n",
        "#import cv2\n",
        "#new_image = cv2.imread('/content/drive/MyDrive/Garbage/Test/Bill.jpg')\n",
        "#plt.imshow(new_image)\n",
        "#new_image.shape\n",
        "\n",
        "\n",
        "# Print the predicted class\n",
        "print(f\"The waste in the image is classified as: {predicted_class}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX9RaPc65N_7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
